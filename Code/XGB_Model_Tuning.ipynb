{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9c717d-8f84-4fab-a771-bce69fc85cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79230c96-0159-433e-afef-a91eef0c5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_os = pd.read_csv('data_train_os.csv')\n",
    "test_os = pd.read_csv('data_test_os.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bbb626-bdee-4c1b-9102-0f5c46f2173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce memory usage of a DataFrame by downcasting numeric columns\n",
    "    and converting object columns to category where appropriate\n",
    "    Datetime columns are preseved(skip)\n",
    "\n",
    "    Paremeters:\n",
    "    - df (pd.DataFrame): The input DataFrame to optimize\n",
    "    - verbose (bool): If True, prints memory usage info\n",
    "                      If False, just execute the operation\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Optimized DataFrame\n",
    "    \"\"\"\n",
    "    # Calculate memory usage before optimization\n",
    "    # 'deep=True' ensures object-type columns (e.g. strings) are fully measured\n",
    "    # Divide by 1024^2 to convert bytes to megabytes\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    # Iterate through each columns in the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Skip datetime columns to avoid corrupting temporal data\n",
    "        if pd.api.types.is_datetime64_any_dtype(col_type):\n",
    "            continue\n",
    "\n",
    "        # If the column is numeric(integer or float)\n",
    "        elif pd.api.types.is_numeric_dtype(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            # If the column is an integer type\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                # Try downcasting to the smallest possible integer type\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "\n",
    "            # If the column is an float type\n",
    "            else:\n",
    "                # Try downcasting to float16, float32, or keep as float64\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                #elif c_min >= np.finfo(np.float64).min and c_max <= np.finfo(np.float64).max:\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        # If the column is an object type\n",
    "        elif pd.api.types.is_object_dtype(col_type):\n",
    "            num_unique = df[col].nunique()\n",
    "            num_total = len(df[col])\n",
    "            # if the unique ratio is below 50%, convert to category for memory\n",
    "            if num_unique / num_total < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "        # Other types are not modified\n",
    "\n",
    "    # Calculate memory usage after optimization\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    if verbose:\n",
    "        # Print summary of memory usage before and after optimization\n",
    "        print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "        print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "        print(f\"Reduced by: {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b01af8e-9d93-4c1f-bbd3-b42c959c7e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 223.87 MB\n",
      "Memory usage after optimization: 53.53 MB\n",
      "Reduced by: 76.1%\n",
      "Memory usage before optimization: 68.66 MB\n",
      "Memory usage after optimization: 16.59 MB\n",
      "Reduced by: 75.8%\n"
     ]
    }
   ],
   "source": [
    "data_os = reduce_mem_usage(data_os)\n",
    "test_os = reduce_mem_usage(test_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bde5f34-e6d4-4cf2-ad45-dc3990fa57bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637893,) (637893, 45) (200000, 45)\n"
     ]
    }
   ],
   "source": [
    "y_os = data_os['isDefault']\n",
    "X_os = data_os.drop(['isDefault'], axis=1)\n",
    "print(y_os.shape, X_os.shape, test_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d36e462-c23e-4899-b048-e1d9a5b5d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# --- Configuration ---\n",
    "N_SPLITS_CV = 5       # Number of folds for cross-validation\n",
    "N_TRIALS_PER_STAGE = 50 # Number of Optuna trials per stage (adjust as needed)\n",
    "OPTUNA_N_JOBS = 5    # Number of parallel jobs for Optuna (-1 uses all cores, adjust if needed)\n",
    "XGB_N_JOBS = 5       # Number of parallel threads for XGBoost model training\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1b3af-fe9c-4fd0-afea-3e24939a2a81",
   "metadata": {},
   "source": [
    "### Stage 1: Tune Tree Structure Parameters (max_depth, min_child_weight, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188f6093-15c8-47b3-8f47-0c3d9b90c8f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 20:54:12,519] A new study created in memory with name: no-name-2954307d-dd6b-49fd-a2df-3f003d522818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 1: Tuning max_depth, min_child_weight, gamma ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abe0d45ce1647b8a289f7c2aae53578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 21:01:56,471] Trial 2 finished with value: 0.7284397905713368 and parameters: {'max_depth': 10, 'min_child_weight': 6.6637722516123965, 'gamma': 3.7713014312115902}. Best is trial 2 with value: 0.7284397905713368.\n",
      "[I 2025-05-15 21:02:33,717] Trial 0 finished with value: 0.7272144607443277 and parameters: {'max_depth': 10, 'min_child_weight': 1.204221982985744, 'gamma': 0.16761875177925323}. Best is trial 2 with value: 0.7284397905713368.\n",
      "[I 2025-05-15 21:03:31,349] Trial 3 finished with value: 0.7312161125589098 and parameters: {'max_depth': 7, 'min_child_weight': 3.081866452662735, 'gamma': 4.461573741846035}. Best is trial 3 with value: 0.7312161125589098.\n",
      "[I 2025-05-15 21:03:36,173] Trial 4 finished with value: 0.7311251989682902 and parameters: {'max_depth': 7, 'min_child_weight': 2.917231132209023, 'gamma': 3.463421854229179}. Best is trial 3 with value: 0.7312161125589098.\n",
      "[I 2025-05-15 21:07:53,080] Trial 8 finished with value: 0.7276398337808245 and parameters: {'max_depth': 10, 'min_child_weight': 3.3804055756557303, 'gamma': 1.7227901888636599}. Best is trial 3 with value: 0.7312161125589098.\n",
      "[I 2025-05-15 21:08:08,531] Trial 7 finished with value: 0.7295926705761357 and parameters: {'max_depth': 9, 'min_child_weight': 9.580770473806577, 'gamma': 0.07444208308902855}. Best is trial 3 with value: 0.7312161125589098.\n",
      "[I 2025-05-15 21:08:47,187] Trial 1 finished with value: 0.7308914278573322 and parameters: {'max_depth': 3, 'min_child_weight': 1.9203080298644986, 'gamma': 4.849764986352392}. Best is trial 3 with value: 0.7312161125589098.\n",
      "[I 2025-05-15 21:09:11,647] Trial 6 finished with value: 0.7318709467392047 and parameters: {'max_depth': 6, 'min_child_weight': 1.0705411515345156, 'gamma': 0.12829069113369562}. Best is trial 6 with value: 0.7318709467392047.\n",
      "[I 2025-05-15 21:11:33,215] Trial 9 finished with value: 0.7302091176368047 and parameters: {'max_depth': 8, 'min_child_weight': 3.7661704245209617, 'gamma': 3.4573509650991503}. Best is trial 6 with value: 0.7318709467392047.\n",
      "[I 2025-05-15 21:11:55,992] Trial 10 finished with value: 0.7303016128966255 and parameters: {'max_depth': 8, 'min_child_weight': 6.257864512313585, 'gamma': 4.640955255786352}. Best is trial 6 with value: 0.7318709467392047.\n",
      "[I 2025-05-15 21:12:22,838] Trial 5 finished with value: 0.7329069915954141 and parameters: {'max_depth': 4, 'min_child_weight': 3.759752735258052, 'gamma': 0.23519810318192613}. Best is trial 5 with value: 0.7329069915954141.\n",
      "[I 2025-05-15 21:12:52,054] Trial 12 finished with value: 0.7302758702708921 and parameters: {'max_depth': 8, 'min_child_weight': 2.018487385059258, 'gamma': 4.635049251178397}. Best is trial 5 with value: 0.7329069915954141.\n",
      "[I 2025-05-15 21:14:31,355] Trial 11 finished with value: 0.7321344032032191 and parameters: {'max_depth': 5, 'min_child_weight': 4.508616800655462, 'gamma': 3.660723109684591}. Best is trial 5 with value: 0.7329069915954141.\n",
      "[I 2025-05-15 21:18:21,980] Trial 13 finished with value: 0.7324456717851727 and parameters: {'max_depth': 3, 'min_child_weight': 7.887719591086849, 'gamma': 1.88767721147238}. Best is trial 5 with value: 0.7329069915954141.\n",
      "[I 2025-05-15 21:18:56,274] Trial 14 finished with value: 0.732871398433774 and parameters: {'max_depth': 4, 'min_child_weight': 1.011058900873677, 'gamma': 1.3808619186698015}. Best is trial 5 with value: 0.7329069915954141.\n",
      "[I 2025-05-15 21:19:40,732] Trial 15 finished with value: 0.7329616987667225 and parameters: {'max_depth': 4, 'min_child_weight': 1.0388238283219966, 'gamma': 1.262805631526134}. Best is trial 15 with value: 0.7329616987667225.\n",
      "[I 2025-05-15 21:19:55,921] Trial 16 finished with value: 0.733011869267736 and parameters: {'max_depth': 4, 'min_child_weight': 1.123473602060215, 'gamma': 1.3432017845835642}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:20:58,588] Trial 17 finished with value: 0.7328389332248577 and parameters: {'max_depth': 4, 'min_child_weight': 5.182690893295158, 'gamma': 1.8529276796860172}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:24:38,847] Trial 20 finished with value: 0.7324897319324819 and parameters: {'max_depth': 5, 'min_child_weight': 2.069803490433769, 'gamma': 0.8604434935827052}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:24:40,409] Trial 21 finished with value: 0.7325830489332975 and parameters: {'max_depth': 5, 'min_child_weight': 1.4849099756856192, 'gamma': 0.9648567198258631}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:24:45,642] Trial 18 finished with value: 0.7324403549060872 and parameters: {'max_depth': 3, 'min_child_weight': 5.839899731490227, 'gamma': 1.8756651701259863}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:25:27,332] Trial 22 finished with value: 0.7325774057568984 and parameters: {'max_depth': 5, 'min_child_weight': 1.4730520680829464, 'gamma': 0.8920700569262341}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:25:44,137] Trial 19 finished with value: 0.7328794579533375 and parameters: {'max_depth': 4, 'min_child_weight': 1.7631638861100225, 'gamma': 1.046528657951602}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:28:23,428] Trial 24 finished with value: 0.7319555744652575 and parameters: {'max_depth': 6, 'min_child_weight': 1.5046660854511436, 'gamma': 2.630610342850737}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:29:06,247] Trial 27 finished with value: 0.7317700800101736 and parameters: {'max_depth': 6, 'min_child_weight': 2.3184743966713524, 'gamma': 2.4691292514187944}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:29:25,832] Trial 23 finished with value: 0.732605756889824 and parameters: {'max_depth': 5, 'min_child_weight': 1.4325904127624363, 'gamma': 2.670891698302832}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:31:28,238] Trial 25 finished with value: 0.7329791906524645 and parameters: {'max_depth': 4, 'min_child_weight': 1.4614338846495205, 'gamma': 0.7677904758156734}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:32:11,589] Trial 26 finished with value: 0.7327376925463036 and parameters: {'max_depth': 4, 'min_child_weight': 2.5887182524285333, 'gamma': 2.5791284465542494}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:35:30,813] Trial 28 finished with value: 0.7327403467020968 and parameters: {'max_depth': 4, 'min_child_weight': 2.5157689784340667, 'gamma': 2.48790078850506}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:35:44,754] Trial 29 finished with value: 0.7329909395731153 and parameters: {'max_depth': 4, 'min_child_weight': 1.2531130892900466, 'gamma': 0.531794126909783}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:36:36,165] Trial 30 finished with value: 0.7329776300638203 and parameters: {'max_depth': 4, 'min_child_weight': 2.620371848861148, 'gamma': 0.4196910424207862}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:38:10,795] Trial 31 finished with value: 0.7323994567774481 and parameters: {'max_depth': 3, 'min_child_weight': 1.222621471517854, 'gamma': 0.6313895917600815}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:38:53,437] Trial 32 finished with value: 0.7323458784723106 and parameters: {'max_depth': 3, 'min_child_weight': 1.2571346602232978, 'gamma': 1.3848817215465918}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:42:15,707] Trial 33 finished with value: 0.7323670602092526 and parameters: {'max_depth': 3, 'min_child_weight': 1.2163600880414935, 'gamma': 0.5671694489179657}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:42:27,371] Trial 34 finished with value: 0.7323385597547801 and parameters: {'max_depth': 3, 'min_child_weight': 1.2555143129373014, 'gamma': 0.5604732209970575}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:43:19,101] Trial 35 finished with value: 0.7324055128712711 and parameters: {'max_depth': 3, 'min_child_weight': 1.2402711966376452, 'gamma': 0.4858019000001106}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:44:52,340] Trial 36 finished with value: 0.7323357767516094 and parameters: {'max_depth': 3, 'min_child_weight': 1.3312653354792654, 'gamma': 0.5583107296191092}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:45:31,602] Trial 37 finished with value: 0.7323512739724054 and parameters: {'max_depth': 3, 'min_child_weight': 1.7176461946999393, 'gamma': 0.5042295315059285}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:47:10,585] Trial 38 finished with value: 0.7324819337341305 and parameters: {'max_depth': 5, 'min_child_weight': 1.6675823921184179, 'gamma': 0.3855306669344931}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:47:45,754] Trial 39 finished with value: 0.7325804415510447 and parameters: {'max_depth': 5, 'min_child_weight': 1.7565269694976684, 'gamma': 0.42471976354346264}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:48:14,512] Trial 40 finished with value: 0.7324655058457376 and parameters: {'max_depth': 5, 'min_child_weight': 1.7364518748777065, 'gamma': 0.23044886267776243}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:50:12,978] Trial 41 finished with value: 0.7326126194845027 and parameters: {'max_depth': 5, 'min_child_weight': 1.741997443134988, 'gamma': 0.2875307026703595}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:50:45,183] Trial 42 finished with value: 0.7325105393341347 and parameters: {'max_depth': 5, 'min_child_weight': 1.6485129974032873, 'gamma': 0.30650460888626324}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:50:54,906] Trial 43 finished with value: 0.7318591286448048 and parameters: {'max_depth': 6, 'min_child_weight': 1.1101069996225272, 'gamma': 1.5912424726153882}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:51:47,129] Trial 44 finished with value: 0.7320827961176146 and parameters: {'max_depth': 6, 'min_child_weight': 1.0906141833579626, 'gamma': 1.6172949035264192}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:55:05,372] Trial 45 finished with value: 0.7329312097672563 and parameters: {'max_depth': 4, 'min_child_weight': 1.0789092571995422, 'gamma': 1.1118804241729712}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:57:04,713] Trial 46 finished with value: 0.7328242694972806 and parameters: {'max_depth': 4, 'min_child_weight': 1.1472522996049737, 'gamma': 1.275647088816942}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:57:13,034] Trial 48 finished with value: 0.7328717831310917 and parameters: {'max_depth': 4, 'min_child_weight': 1.1050190674777713, 'gamma': 1.211348487161683}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:57:21,704] Trial 47 finished with value: 0.7328270902904446 and parameters: {'max_depth': 4, 'min_child_weight': 1.0793279714027928, 'gamma': 1.243174033684802}. Best is trial 16 with value: 0.733011869267736.\n",
      "[I 2025-05-15 21:57:56,538] Trial 49 finished with value: 0.7329048140090606 and parameters: {'max_depth': 4, 'min_child_weight': 1.0020755272287682, 'gamma': 2.1392382249825417}. Best is trial 16 with value: 0.733011869267736.\n",
      "✅ Best parameters from Stage 1:\n",
      "{'max_depth': 4, 'min_child_weight': 1.123473602060215, 'gamma': 1.3432017845835642}\n",
      "✅ Best AUC from Stage 1: 0.7330\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Stage 1: Tuning max_depth, min_child_weight, gamma ---\")\n",
    "\n",
    "def objective_stage1(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', # For binary classification\n",
    "        'eval_metric': 'auc',           # Metric for early stopping, consistent with Optuna's direction\n",
    "        'booster': 'gbtree',\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': XGB_N_JOBS,\n",
    "        # 'tree_method': 'hist', # Consider 'hist' for faster training on large datasets, or 'gpu_hist' for GPU\n",
    "\n",
    "        # Parameters to tune in Stage 1\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5, log=False), # often 0 to 0.5 or higher if needed\n",
    "\n",
    "        # Fixed for this stage (will be tuned later or determined by early stopping)\n",
    "        'learning_rate': 0.1, # Relatively high learning rate for faster exploration\n",
    "        'n_estimators': 1000, # High number, early stopping will find the optimum\n",
    "\n",
    "        # Default values for parameters tuned in later stages\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.0, # L1 regularization\n",
    "        'reg_lambda': 1.0,# L2 regularization (XGBoost default is 1)\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=RANDOM_SEED)\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_os, y_os):\n",
    "        X_train_fold, X_valid_fold = X_os.iloc[train_idx], X_os.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y_os.iloc[train_idx], y_os.iloc[valid_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                  verbose=False) # Suppress XGBoost training output during Optuna trials\n",
    "        \n",
    "        # model.best_iteration can be used if n_estimators is what you're optimizing\n",
    "        preds_proba = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_valid_fold, preds_proba)\n",
    "        aucs.append(auc)\n",
    "        \n",
    "        # Optuna Pruning (optional, but recommended for longer stages)\n",
    "        # trial.report(auc, len(aucs)) # Report intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return np.mean(aucs)\n",
    "\n",
    "study_stage1 = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_stage1.optimize(objective_stage1, n_trials=N_TRIALS_PER_STAGE, n_jobs=OPTUNA_N_JOBS, show_progress_bar=True)\n",
    "\n",
    "best_params_stage1 = study_stage1.best_params\n",
    "print(\"✅ Best parameters from Stage 1:\")\n",
    "print(best_params_stage1)\n",
    "print(f\"✅ Best AUC from Stage 1: {study_stage1.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b95e97-53e8-454d-96b0-1e8fa5f0b3a8",
   "metadata": {},
   "source": [
    "### Stage 2: Tune Regularization Parameters (reg_alpha, reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf49230a-d601-4e34-bdec-692192ef022c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 21:57:56,553] A new study created in memory with name: no-name-3ae8d25a-b68a-4a76-b5de-c02ef662f30b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 2: Tuning reg_alpha, reg_lambda ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc803772a64f477c96c61b8e4d6aeb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 22:04:34,885] Trial 1 finished with value: 0.7325666642569799 and parameters: {'reg_alpha': 0.013047368428776506, 'reg_lambda': 0.010636557353793534}. Best is trial 1 with value: 0.7325666642569799.\n",
      "[I 2025-05-15 22:04:49,570] Trial 0 finished with value: 0.7328326558681195 and parameters: {'reg_alpha': 0.685782950220429, 'reg_lambda': 0.08441600456996912}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:04:55,786] Trial 2 finished with value: 0.7326926818529526 and parameters: {'reg_alpha': 0.0035806907907943207, 'reg_lambda': 0.380825192327806}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:05:00,047] Trial 3 finished with value: 0.7327399366209594 and parameters: {'reg_alpha': 0.03613145662663032, 'reg_lambda': 0.8391759968881708}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:05:15,225] Trial 4 finished with value: 0.7327326687005788 and parameters: {'reg_alpha': 0.24674116098022147, 'reg_lambda': 0.014303936618555425}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:11:20,476] Trial 5 finished with value: 0.7327111104503636 and parameters: {'reg_alpha': 0.028976258898464103, 'reg_lambda': 0.005347563171613238}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:11:30,774] Trial 6 finished with value: 0.732621097997218 and parameters: {'reg_alpha': 0.0162231705645332, 'reg_lambda': 0.0039188674123017825}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:11:46,475] Trial 8 finished with value: 0.7327056572903599 and parameters: {'reg_alpha': 0.06309570963183608, 'reg_lambda': 0.0811537882775526}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:11:47,407] Trial 7 finished with value: 0.7326887901579319 and parameters: {'reg_alpha': 0.117244611443891, 'reg_lambda': 0.08321589956617476}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:11:51,402] Trial 9 finished with value: 0.732694444176061 and parameters: {'reg_alpha': 0.14018370547985937, 'reg_lambda': 0.0026681733172185243}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:18:55,351] Trial 11 finished with value: 0.7327605791603137 and parameters: {'reg_alpha': 0.005656223800308024, 'reg_lambda': 0.004037528587759853}. Best is trial 0 with value: 0.7328326558681195.\n",
      "[I 2025-05-15 22:19:04,061] Trial 10 finished with value: 0.7329299771826644 and parameters: {'reg_alpha': 0.01905303832207982, 'reg_lambda': 0.39369461314240206}. Best is trial 10 with value: 0.7329299771826644.\n",
      "[I 2025-05-15 22:19:16,611] Trial 12 finished with value: 0.7328067393369146 and parameters: {'reg_alpha': 0.429294615296271, 'reg_lambda': 0.00277923888750164}. Best is trial 10 with value: 0.7329299771826644.\n",
      "[I 2025-05-15 22:19:22,577] Trial 13 finished with value: 0.7328977305890164 and parameters: {'reg_alpha': 0.8658767091210485, 'reg_lambda': 0.09213320757230482}. Best is trial 10 with value: 0.7329299771826644.\n",
      "[I 2025-05-15 22:19:27,917] Trial 14 finished with value: 0.732860144918515 and parameters: {'reg_alpha': 0.8896918080143933, 'reg_lambda': 0.0010495804148885307}. Best is trial 10 with value: 0.7329299771826644.\n",
      "[I 2025-05-15 22:26:49,918] Trial 16 finished with value: 0.7328628302019116 and parameters: {'reg_alpha': 0.8604279325039301, 'reg_lambda': 0.15676423096683473}. Best is trial 10 with value: 0.7329299771826644.\n",
      "[I 2025-05-15 22:27:10,390] Trial 15 finished with value: 0.7328195706091757 and parameters: {'reg_alpha': 0.6528843620730296, 'reg_lambda': 0.0011178352259592602}. Best is trial 10 with value: 0.7329299771826644.\n",
      "[I 2025-05-15 22:27:12,621] Trial 18 finished with value: 0.7327626323800831 and parameters: {'reg_alpha': 0.008156435542263477, 'reg_lambda': 0.2620562141741352}. Best is trial 10 with value: 0.7329299771826644.\n",
      "[I 2025-05-15 22:27:21,096] Trial 17 finished with value: 0.7331211863884978 and parameters: {'reg_alpha': 0.9892855839750394, 'reg_lambda': 0.2097958169371025}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:27:28,152] Trial 19 finished with value: 0.7327494397282381 and parameters: {'reg_alpha': 0.0016460803555942468, 'reg_lambda': 0.25616671042365863}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:34:13,788] Trial 20 finished with value: 0.7329617274813234 and parameters: {'reg_alpha': 0.0012614431233103265, 'reg_lambda': 0.29967624145470845}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:34:39,430] Trial 21 finished with value: 0.7328307854353763 and parameters: {'reg_alpha': 0.008366652976831125, 'reg_lambda': 0.4215312655407657}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:34:45,969] Trial 22 finished with value: 0.7329517151748487 and parameters: {'reg_alpha': 0.0015186090196258706, 'reg_lambda': 0.9920050726088336}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:35:21,779] Trial 23 finished with value: 0.7330003261719649 and parameters: {'reg_alpha': 0.0018248307992219741, 'reg_lambda': 0.9970886575882377}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:35:30,342] Trial 24 finished with value: 0.7328925573305076 and parameters: {'reg_alpha': 0.05550106959272151, 'reg_lambda': 0.8262898294591711}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:42:30,416] Trial 25 finished with value: 0.7329775429000376 and parameters: {'reg_alpha': 0.0011755818040468011, 'reg_lambda': 0.992204242932431}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:42:50,926] Trial 27 finished with value: 0.732896491861486 and parameters: {'reg_alpha': 0.0012068864371046525, 'reg_lambda': 0.9623165480116569}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:43:06,309] Trial 26 finished with value: 0.7329654405202201 and parameters: {'reg_alpha': 0.0034819506202869712, 'reg_lambda': 0.6749335222994054}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:43:21,288] Trial 29 finished with value: 0.7325775261109235 and parameters: {'reg_alpha': 0.0011347593746601013, 'reg_lambda': 0.03883397170990696}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:43:25,981] Trial 28 finished with value: 0.7324933921195578 and parameters: {'reg_alpha': 0.002686903656547598, 'reg_lambda': 0.03506679218655603}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:48:57,388] Trial 30 finished with value: 0.7328619160785677 and parameters: {'reg_alpha': 0.002209401800511458, 'reg_lambda': 0.584109466690386}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:49:13,541] Trial 31 finished with value: 0.7327411994188676 and parameters: {'reg_alpha': 0.002536730343548724, 'reg_lambda': 0.04050014721143347}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:49:21,684] Trial 32 finished with value: 0.7326113392602054 and parameters: {'reg_alpha': 0.002282484995698748, 'reg_lambda': 0.03090774858696839}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:49:41,447] Trial 34 finished with value: 0.7327523142285834 and parameters: {'reg_alpha': 0.0020003885892914313, 'reg_lambda': 0.18394383428444813}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:49:52,308] Trial 33 finished with value: 0.7326430599016511 and parameters: {'reg_alpha': 0.002366291680672847, 'reg_lambda': 0.1669896887709312}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:56:00,748] Trial 35 finished with value: 0.73261436409245 and parameters: {'reg_alpha': 0.0044034634720909824, 'reg_lambda': 0.17026484990535398}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:56:25,693] Trial 36 finished with value: 0.7326492635987021 and parameters: {'reg_alpha': 0.0045727089889189, 'reg_lambda': 0.1660706957085924}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:56:47,232] Trial 37 finished with value: 0.7328444535016051 and parameters: {'reg_alpha': 0.004172135151563052, 'reg_lambda': 0.5824961936722718}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:57:00,952] Trial 39 finished with value: 0.732899843426225 and parameters: {'reg_alpha': 0.0043833083862237655, 'reg_lambda': 0.5814281090881607}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 22:57:12,637] Trial 38 finished with value: 0.7328359588849889 and parameters: {'reg_alpha': 0.004288545190643262, 'reg_lambda': 0.5411433514255949}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:02:49,591] Trial 40 finished with value: 0.7328541785163845 and parameters: {'reg_alpha': 0.003676898522465021, 'reg_lambda': 0.5886961970605512}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:03:30,439] Trial 41 finished with value: 0.7330197688356751 and parameters: {'reg_alpha': 0.00823940077444156, 'reg_lambda': 0.5643607005943031}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:03:45,219] Trial 42 finished with value: 0.7327860308358756 and parameters: {'reg_alpha': 0.00863965194007594, 'reg_lambda': 0.5590330239465715}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:03:45,567] Trial 43 finished with value: 0.7327514965276287 and parameters: {'reg_alpha': 0.009571229200114509, 'reg_lambda': 0.5949324435141843}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:04:24,571] Trial 44 finished with value: 0.7329106610349898 and parameters: {'reg_alpha': 0.008824152433111316, 'reg_lambda': 0.7073317769318395}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:10:44,960] Trial 45 finished with value: 0.732735445981518 and parameters: {'reg_alpha': 0.0010103244277008942, 'reg_lambda': 0.29193697073281644}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:10:59,757] Trial 46 finished with value: 0.7329404990639755 and parameters: {'reg_alpha': 0.011442259012262325, 'reg_lambda': 0.750354602440156}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:11:00,965] Trial 48 finished with value: 0.7326250695350035 and parameters: {'reg_alpha': 0.014878845060392316, 'reg_lambda': 0.38752817134724005}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:11:22,960] Trial 47 finished with value: 0.7327207770814711 and parameters: {'reg_alpha': 0.01355716424277293, 'reg_lambda': 0.3577336297616916}. Best is trial 17 with value: 0.7331211863884978.\n",
      "[I 2025-05-15 23:11:35,562] Trial 49 finished with value: 0.7328525903064481 and parameters: {'reg_alpha': 0.02919670751995637, 'reg_lambda': 0.3531179433419193}. Best is trial 17 with value: 0.7331211863884978.\n",
      "✅ Best parameters from Stage 2 (alpha, lambda):\n",
      "{'reg_alpha': 0.9892855839750394, 'reg_lambda': 0.2097958169371025}\n",
      "✅ Best AUC from Stage 2: 0.7331\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Stage 2: Tuning reg_alpha, reg_lambda ---\")\n",
    "\n",
    "def objective_stage2(trial):\n",
    "    # Start with best parameters from Stage 1\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': 'gbtree',\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': XGB_N_JOBS,\n",
    "        # 'tree_method': 'hist',\n",
    "\n",
    "        # Parameters from Stage 1 (fixed)\n",
    "        'max_depth': best_params_stage1['max_depth'],\n",
    "        'min_child_weight': best_params_stage1['min_child_weight'],\n",
    "        'gamma': best_params_stage1['gamma'],\n",
    "\n",
    "        # Parameters to tune in Stage 2\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),\n",
    "\n",
    "        # Fixed for this stage\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 1000,\n",
    "\n",
    "        # Default values for parameters tuned in later stages\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=RANDOM_SEED)\n",
    "    aucs = []\n",
    "    for train_idx, valid_idx in cv.split(X_os, y_os):\n",
    "        X_train_fold, X_valid_fold = X_os.iloc[train_idx], X_os.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y_os.iloc[train_idx], y_os.iloc[valid_idx]\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                  verbose=False)\n",
    "        preds_proba = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_valid_fold, preds_proba)\n",
    "        aucs.append(auc)\n",
    "    return np.mean(aucs)\n",
    "\n",
    "study_stage2 = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_stage2.optimize(objective_stage2, n_trials=N_TRIALS_PER_STAGE, n_jobs=OPTUNA_N_JOBS, show_progress_bar=True)\n",
    "\n",
    "best_params_stage2 = study_stage2.best_params\n",
    "# Combine with previous stage best params\n",
    "current_best_params = best_params_stage1.copy()\n",
    "current_best_params.update(best_params_stage2)\n",
    "\n",
    "print(\"✅ Best parameters from Stage 2 (alpha, lambda):\")\n",
    "print(best_params_stage2)\n",
    "print(f\"✅ Best AUC from Stage 2: {study_stage2.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d790d6c-daa6-4c29-a503-32b02d360438",
   "metadata": {},
   "source": [
    "### Stage 3: Tune Subsampling Parameters (subsample, colsample_bytree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19406177-4c14-485a-8462-d95354580c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 23:13:50,535] A new study created in memory with name: no-name-363f8f1c-8134-48d4-92a5-8665abef0ea3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 3: Tuning subsample, colsample_bytree ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab0271e482d43e8a702c735cc9e6d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 23:19:10,580] Trial 0 finished with value: 0.7319845187363054 and parameters: {'subsample': 0.9964490976318493, 'colsample_bytree': 0.9725121221361984}. Best is trial 0 with value: 0.7319845187363054.\n",
      "[I 2025-05-15 23:20:48,265] Trial 3 finished with value: 0.732285516213359 and parameters: {'subsample': 0.5040592132091599, 'colsample_bytree': 0.9549383616292366}. Best is trial 3 with value: 0.732285516213359.\n",
      "[I 2025-05-15 23:20:48,701] Trial 2 finished with value: 0.7323496236785866 and parameters: {'subsample': 0.5519305123353417, 'colsample_bytree': 0.9173327233402682}. Best is trial 2 with value: 0.7323496236785866.\n",
      "[I 2025-05-15 23:21:21,805] Trial 1 finished with value: 0.7327112051714566 and parameters: {'subsample': 0.6848168517523919, 'colsample_bytree': 0.66897114616099}. Best is trial 1 with value: 0.7327112051714566.\n",
      "[I 2025-05-15 23:21:57,748] Trial 4 finished with value: 0.7327908501224126 and parameters: {'subsample': 0.7281928806929132, 'colsample_bytree': 0.9532278282471938}. Best is trial 4 with value: 0.7327908501224126.\n",
      "[I 2025-05-15 23:25:56,030] Trial 5 finished with value: 0.7319768075361642 and parameters: {'subsample': 0.9902618319600629, 'colsample_bytree': 0.5080862586064401}. Best is trial 4 with value: 0.7327908501224126.\n",
      "[I 2025-05-15 23:29:09,275] Trial 7 finished with value: 0.7331146174975877 and parameters: {'subsample': 0.8570335911436417, 'colsample_bytree': 0.9596447389346743}. Best is trial 7 with value: 0.7331146174975877.\n",
      "[I 2025-05-15 23:29:14,551] Trial 6 finished with value: 0.7327635239445209 and parameters: {'subsample': 0.7396857478809266, 'colsample_bytree': 0.7131827511047337}. Best is trial 7 with value: 0.7331146174975877.\n",
      "[I 2025-05-15 23:29:28,975] Trial 8 finished with value: 0.7328737421122667 and parameters: {'subsample': 0.7899622596081086, 'colsample_bytree': 0.91233168765143}. Best is trial 7 with value: 0.7331146174975877.\n",
      "[I 2025-05-15 23:29:34,636] Trial 9 finished with value: 0.7327434751892851 and parameters: {'subsample': 0.9654831854864168, 'colsample_bytree': 0.9912274917516402}. Best is trial 7 with value: 0.7331146174975877.\n",
      "[I 2025-05-15 23:33:09,191] Trial 10 finished with value: 0.7331669781699924 and parameters: {'subsample': 0.8248359344529724, 'colsample_bytree': 0.9357640060418342}. Best is trial 10 with value: 0.7331669781699924.\n",
      "[I 2025-05-15 23:35:47,641] Trial 11 finished with value: 0.7330297982242936 and parameters: {'subsample': 0.860771840441666, 'colsample_bytree': 0.5087319748725578}. Best is trial 10 with value: 0.7331669781699924.\n",
      "[I 2025-05-15 23:35:50,611] Trial 13 finished with value: 0.7327607032677812 and parameters: {'subsample': 0.6688101962224213, 'colsample_bytree': 0.9887844537881194}. Best is trial 10 with value: 0.7331669781699924.\n",
      "[I 2025-05-15 23:35:54,339] Trial 12 finished with value: 0.7330470774955432 and parameters: {'subsample': 0.8423110078148437, 'colsample_bytree': 0.6474787726785195}. Best is trial 10 with value: 0.7331669781699924.\n",
      "[I 2025-05-15 23:36:30,384] Trial 14 finished with value: 0.7331792030964552 and parameters: {'subsample': 0.8481184627170759, 'colsample_bytree': 0.8216583715824572}. Best is trial 14 with value: 0.7331792030964552.\n",
      "[I 2025-05-15 23:39:28,781] Trial 15 finished with value: 0.7331556859271295 and parameters: {'subsample': 0.8577820374566804, 'colsample_bytree': 0.8018592112884564}. Best is trial 14 with value: 0.7331792030964552.\n",
      "[I 2025-05-15 23:42:01,603] Trial 16 finished with value: 0.7331113940728335 and parameters: {'subsample': 0.8718338776671285, 'colsample_bytree': 0.8219183609896312}. Best is trial 14 with value: 0.7331792030964552.\n",
      "[I 2025-05-15 23:42:02,243] Trial 17 finished with value: 0.7330138125809069 and parameters: {'subsample': 0.8713419587303516, 'colsample_bytree': 0.8321668382581948}. Best is trial 14 with value: 0.7331792030964552.\n",
      "[I 2025-05-15 23:42:06,378] Trial 18 finished with value: 0.7332102835301088 and parameters: {'subsample': 0.8898585993283262, 'colsample_bytree': 0.8193222521044138}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-15 23:42:34,537] Trial 19 finished with value: 0.7329399022197398 and parameters: {'subsample': 0.9122131659209045, 'colsample_bytree': 0.8222632024940472}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-15 23:45:32,990] Trial 20 finished with value: 0.7329469660344593 and parameters: {'subsample': 0.9298358447131149, 'colsample_bytree': 0.8207179453184559}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-15 23:48:23,136] Trial 23 finished with value: 0.7330608861352869 and parameters: {'subsample': 0.9322162730481174, 'colsample_bytree': 0.8716972129719871}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-15 23:48:27,574] Trial 22 finished with value: 0.7330192455501778 and parameters: {'subsample': 0.9393700811369956, 'colsample_bytree': 0.880318775638367}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-15 23:48:37,706] Trial 21 finished with value: 0.733002572890528 and parameters: {'subsample': 0.9207836225253965, 'colsample_bytree': 0.8418932562311292}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-15 23:49:02,616] Trial 24 finished with value: 0.7330715779657219 and parameters: {'subsample': 0.9304397452585431, 'colsample_bytree': 0.8644913790448447}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 05:57:22,274] Trial 25 finished with value: 0.7329103873763166 and parameters: {'subsample': 0.7918506491268164, 'colsample_bytree': 0.8781084754488951}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:02:26,388] Trial 26 finished with value: 0.7328899072732605 and parameters: {'subsample': 0.8010362174292837, 'colsample_bytree': 0.7578809597581209}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:02:43,398] Trial 27 finished with value: 0.7329717805028183 and parameters: {'subsample': 0.7630747194355777, 'colsample_bytree': 0.7588177480494936}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:02:53,944] Trial 28 finished with value: 0.7331485590053924 and parameters: {'subsample': 0.809327804070442, 'colsample_bytree': 0.7671614965092074}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:02:58,055] Trial 29 finished with value: 0.7329606074796402 and parameters: {'subsample': 0.7907206136039905, 'colsample_bytree': 0.767990997625117}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:07:12,560] Trial 30 finished with value: 0.7330077422065047 and parameters: {'subsample': 0.8005017955531866, 'colsample_bytree': 0.7510688158049506}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:10:20,381] Trial 31 finished with value: 0.7331596128668172 and parameters: {'subsample': 0.8191010092111948, 'colsample_bytree': 0.7596883395990414}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:10:35,161] Trial 33 finished with value: 0.7327019057774123 and parameters: {'subsample': 0.6930769281179398, 'colsample_bytree': 0.6946350183990593}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:10:37,583] Trial 32 finished with value: 0.7329719916137856 and parameters: {'subsample': 0.8260140104506546, 'colsample_bytree': 0.781962875946945}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:11:01,231] Trial 34 finished with value: 0.7331895770142429 and parameters: {'subsample': 0.8965280229757232, 'colsample_bytree': 0.9191475588313128}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:15:30,325] Trial 35 finished with value: 0.7331078703501445 and parameters: {'subsample': 0.8909905204228903, 'colsample_bytree': 0.6973654137285961}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:18:13,884] Trial 36 finished with value: 0.7329143417782312 and parameters: {'subsample': 0.6998939201646638, 'colsample_bytree': 0.6955423661409172}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:18:52,055] Trial 37 finished with value: 0.7331487413329484 and parameters: {'subsample': 0.8916595625526412, 'colsample_bytree': 0.6462581451915623}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:19:03,682] Trial 38 finished with value: 0.7328626977637028 and parameters: {'subsample': 0.7669596984596905, 'colsample_bytree': 0.6315611034648524}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:19:04,135] Trial 39 finished with value: 0.7331661250186048 and parameters: {'subsample': 0.8883473517100937, 'colsample_bytree': 0.9278364345311187}. Best is trial 18 with value: 0.7332102835301088.\n",
      "[I 2025-05-16 09:23:37,853] Trial 40 finished with value: 0.7332336946705933 and parameters: {'subsample': 0.8980386684264224, 'colsample_bytree': 0.9088349516071941}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:24:55,727] Trial 43 finished with value: 0.7321552604289364 and parameters: {'subsample': 0.9943220541249442, 'colsample_bytree': 0.9153694533195132}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:25:32,109] Trial 41 finished with value: 0.7331808785384835 and parameters: {'subsample': 0.8930630383426887, 'colsample_bytree': 0.918308588965248}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:25:33,928] Trial 42 finished with value: 0.7326598722717844 and parameters: {'subsample': 0.9742953421728087, 'colsample_bytree': 0.9155873517548074}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:25:48,393] Trial 44 finished with value: 0.7323799890534 and parameters: {'subsample': 0.5579206992439152, 'colsample_bytree': 0.9419131789738864}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:29:14,157] Trial 45 finished with value: 0.7321140428099235 and parameters: {'subsample': 0.9953537472864653, 'colsample_bytree': 0.9112103693035103}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:31:13,544] Trial 46 finished with value: 0.7326782054580855 and parameters: {'subsample': 0.9615668739515499, 'colsample_bytree': 0.9394782670471501}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:31:46,400] Trial 47 finished with value: 0.7327839469891466 and parameters: {'subsample': 0.9622967774166716, 'colsample_bytree': 0.9059686562418234}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:32:06,671] Trial 49 finished with value: 0.732887219742618 and parameters: {'subsample': 0.9645804460770891, 'colsample_bytree': 0.8919914536636461}. Best is trial 40 with value: 0.7332336946705933.\n",
      "[I 2025-05-16 09:32:17,941] Trial 48 finished with value: 0.7327329898078665 and parameters: {'subsample': 0.6145739785785259, 'colsample_bytree': 0.9488034264712941}. Best is trial 40 with value: 0.7332336946705933.\n",
      "✅ Best parameters from Stage 3 (subsample, colsample):\n",
      "{'subsample': 0.8980386684264224, 'colsample_bytree': 0.9088349516071941}\n",
      "✅ Best AUC from Stage 3: 0.7332\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Stage 3: Tuning subsample, colsample_bytree ---\")\n",
    "\n",
    "def objective_stage3(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': 'gbtree',\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': XGB_N_JOBS,\n",
    "        # 'tree_method': 'hist',\n",
    "\n",
    "        # Parameters from Stage 1 & 2 (fixed)\n",
    "        'max_depth': current_best_params['max_depth'],\n",
    "        'min_child_weight': current_best_params['min_child_weight'],\n",
    "        'gamma': current_best_params['gamma'],\n",
    "        'reg_alpha': current_best_params['reg_alpha'],\n",
    "        'reg_lambda': current_best_params['reg_lambda'],\n",
    "\n",
    "        # Parameters to tune in Stage 3\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        # Optional: You can also tune colsample_bylevel and colsample_bynode here if desired\n",
    "        # 'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "        # 'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
    "\n",
    "        # Fixed for this stage\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 1000,\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=RANDOM_SEED)\n",
    "    aucs = []\n",
    "    for train_idx, valid_idx in cv.split(X_os, y_os):\n",
    "        X_train_fold, X_valid_fold = X_os.iloc[train_idx], X_os.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y_os.iloc[train_idx], y_os.iloc[valid_idx]\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                  verbose=False)\n",
    "        preds_proba = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_valid_fold, preds_proba)\n",
    "        aucs.append(auc)\n",
    "    return np.mean(aucs)\n",
    "\n",
    "study_stage3 = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_stage3.optimize(objective_stage3, n_trials=N_TRIALS_PER_STAGE, n_jobs=OPTUNA_N_JOBS, show_progress_bar=True)\n",
    "\n",
    "best_params_stage3 = study_stage3.best_params\n",
    "current_best_params.update(best_params_stage3) # Add new best params\n",
    "\n",
    "print(\"✅ Best parameters from Stage 3 (subsample, colsample):\")\n",
    "print(best_params_stage3)\n",
    "print(f\"✅ Best AUC from Stage 3: {study_stage3.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf17b1-091c-4342-9417-124c17c47f77",
   "metadata": {},
   "source": [
    "### Stage 4: Tune Learning Rate (learning_rate) and n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c7c639-49ea-4bf2-811b-7ebfc126f65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-16 11:54:12,526] A new study created in memory with name: no-name-dbc871f7-efee-4279-aaef-28a25bd73d8a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stage 4: Tuning learning_rate (and n_estimators via early stopping) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd0d98ee8f64511a0157a8bdfe7cea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-16 11:59:31,310] Trial 2 finished with value: 0.719483229317824 and parameters: {'learning_rate': 0.00628184936845356, 'n_estimators': 700}. Best is trial 2 with value: 0.719483229317824.\n",
      "[I 2025-05-16 12:01:28,881] Trial 3 finished with value: 0.7245319133916258 and parameters: {'learning_rate': 0.008575595966446624, 'n_estimators': 1000}. Best is trial 3 with value: 0.7245319133916258.\n",
      "[I 2025-05-16 12:01:57,262] Trial 1 finished with value: 0.730185896398741 and parameters: {'learning_rate': 0.022306253682001575, 'n_estimators': 1100}. Best is trial 1 with value: 0.730185896398741.\n",
      "[I 2025-05-16 12:04:27,426] Trial 0 finished with value: 0.732775092598562 and parameters: {'learning_rate': 0.034188699628264, 'n_estimators': 1500}. Best is trial 0 with value: 0.732775092598562.\n",
      "[I 2025-05-16 12:05:58,940] Trial 4 finished with value: 0.7288081148870024 and parameters: {'learning_rate': 0.01054133957517877, 'n_estimators': 1700}. Best is trial 0 with value: 0.732775092598562.\n",
      "[I 2025-05-16 12:06:42,940] Trial 5 finished with value: 0.7331485587271788 and parameters: {'learning_rate': 0.06285875402593002, 'n_estimators': 1100}. Best is trial 5 with value: 0.7331485587271788.\n",
      "[I 2025-05-16 12:12:55,287] Trial 8 finished with value: 0.7333563218435455 and parameters: {'learning_rate': 0.08045353683272784, 'n_estimators': 2000}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:13:22,728] Trial 10 finished with value: 0.7227991055175812 and parameters: {'learning_rate': 0.007346129757823673, 'n_estimators': 900}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:16:15,890] Trial 7 finished with value: 0.7305528092525344 and parameters: {'learning_rate': 0.01271797996812143, 'n_estimators': 2100}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:16:19,175] Trial 6 finished with value: 0.7320552901165689 and parameters: {'learning_rate': 0.018050098201541992, 'n_estimators': 2200}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:20:12,643] Trial 9 finished with value: 0.7326688068971011 and parameters: {'learning_rate': 0.02323955546998316, 'n_estimators': 2100}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:20:49,505] Trial 11 finished with value: 0.7247134229344516 and parameters: {'learning_rate': 0.008051281056384106, 'n_estimators': 1100}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:20:52,616] Trial 12 finished with value: 0.733032829368127 and parameters: {'learning_rate': 0.09041375325231452, 'n_estimators': 1900}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:23:54,628] Trial 14 finished with value: 0.7331224359478707 and parameters: {'learning_rate': 0.09678792155197649, 'n_estimators': 3000}. Best is trial 8 with value: 0.7333563218435455.\n",
      "[I 2025-05-16 12:25:40,388] Trial 13 finished with value: 0.7333721688981918 and parameters: {'learning_rate': 0.07157744016963853, 'n_estimators': 2100}. Best is trial 13 with value: 0.7333721688981918.\n",
      "[I 2025-05-16 12:27:19,407] Trial 15 finished with value: 0.7331332872876922 and parameters: {'learning_rate': 0.09721934696767119, 'n_estimators': 2800}. Best is trial 13 with value: 0.7333721688981918.\n",
      "[I 2025-05-16 12:28:11,580] Trial 16 finished with value: 0.7331706693817732 and parameters: {'learning_rate': 0.09367142143872143, 'n_estimators': 2500}. Best is trial 13 with value: 0.7333721688981918.\n",
      "[I 2025-05-16 12:28:29,367] Trial 17 finished with value: 0.7332379813356494 and parameters: {'learning_rate': 0.09023575363494828, 'n_estimators': 2900}. Best is trial 13 with value: 0.7333721688981918.\n",
      "[I 2025-05-16 12:35:39,515] Trial 18 finished with value: 0.7336119708980272 and parameters: {'learning_rate': 0.05810505055244664, 'n_estimators': 2600}. Best is trial 18 with value: 0.7336119708980272.\n",
      "[I 2025-05-16 12:37:58,200] Trial 21 finished with value: 0.7333174419290676 and parameters: {'learning_rate': 0.050491036678915135, 'n_estimators': 1500}. Best is trial 18 with value: 0.7336119708980272.\n",
      "[I 2025-05-16 12:38:20,621] Trial 19 finished with value: 0.7336094256659174 and parameters: {'learning_rate': 0.0504516461736701, 'n_estimators': 2500}. Best is trial 18 with value: 0.7336119708980272.\n",
      "[I 2025-05-16 12:41:16,712] Trial 20 finished with value: 0.7337347984634943 and parameters: {'learning_rate': 0.05066265158449503, 'n_estimators': 2500}. Best is trial 20 with value: 0.7337347984634943.\n",
      "[I 2025-05-16 12:43:56,439] Trial 22 finished with value: 0.7336896272569441 and parameters: {'learning_rate': 0.045139900600159606, 'n_estimators': 2500}. Best is trial 20 with value: 0.7337347984634943.\n",
      "[I 2025-05-16 12:50:04,106] Trial 23 finished with value: 0.7336466895425391 and parameters: {'learning_rate': 0.04702120572652973, 'n_estimators': 2500}. Best is trial 20 with value: 0.7337347984634943.\n",
      "[I 2025-05-16 12:53:39,634] Trial 25 finished with value: 0.7336176874093703 and parameters: {'learning_rate': 0.04374830432148077, 'n_estimators': 2500}. Best is trial 20 with value: 0.7337347984634943.\n",
      "[I 2025-05-16 12:55:53,865] Trial 24 finished with value: 0.733667325459548 and parameters: {'learning_rate': 0.04452661896566084, 'n_estimators': 2500}. Best is trial 20 with value: 0.7337347984634943.\n",
      "[I 2025-05-16 12:59:12,045] Trial 26 finished with value: 0.733664044397784 and parameters: {'learning_rate': 0.042539692083439676, 'n_estimators': 2600}. Best is trial 20 with value: 0.7337347984634943.\n",
      "[I 2025-05-16 13:04:20,754] Trial 27 finished with value: 0.7337892452284646 and parameters: {'learning_rate': 0.037512263656820664, 'n_estimators': 2600}. Best is trial 27 with value: 0.7337892452284646.\n",
      "[I 2025-05-16 13:09:15,574] Trial 28 finished with value: 0.7338267199650517 and parameters: {'learning_rate': 0.03660137857940386, 'n_estimators': 2400}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:11:36,350] Trial 29 finished with value: 0.7335558288059548 and parameters: {'learning_rate': 0.03334201587379183, 'n_estimators': 2300}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:13:34,584] Trial 30 finished with value: 0.7336653968334799 and parameters: {'learning_rate': 0.033774250213358246, 'n_estimators': 2300}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:16:36,312] Trial 31 finished with value: 0.7335869874516207 and parameters: {'learning_rate': 0.032928348544039344, 'n_estimators': 2300}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:21:06,306] Trial 32 finished with value: 0.7335589558040986 and parameters: {'learning_rate': 0.0328687683298643, 'n_estimators': 2300}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:26:02,460] Trial 33 finished with value: 0.7335266649914711 and parameters: {'learning_rate': 0.0318942927902993, 'n_estimators': 2300}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:31:14,626] Trial 34 finished with value: 0.7338152860513967 and parameters: {'learning_rate': 0.030641292508987047, 'n_estimators': 2800}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:33:14,333] Trial 35 finished with value: 0.7337089116357258 and parameters: {'learning_rate': 0.027794036528260853, 'n_estimators': 2800}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:36:11,090] Trial 36 finished with value: 0.733649216674868 and parameters: {'learning_rate': 0.026079138996365138, 'n_estimators': 2800}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:40:28,664] Trial 37 finished with value: 0.7335234111721594 and parameters: {'learning_rate': 0.025012555068305086, 'n_estimators': 2800}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:44:21,405] Trial 38 finished with value: 0.7333372269934038 and parameters: {'learning_rate': 0.023853548183780503, 'n_estimators': 2700}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:50:18,551] Trial 39 finished with value: 0.7335752231927773 and parameters: {'learning_rate': 0.025460666849641177, 'n_estimators': 2800}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:52:17,974] Trial 40 finished with value: 0.7335942159255591 and parameters: {'learning_rate': 0.025269699087485584, 'n_estimators': 2800}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:56:08,539] Trial 43 finished with value: 0.7309747722871223 and parameters: {'learning_rate': 0.017233984256175793, 'n_estimators': 1700}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:56:46,181] Trial 41 finished with value: 0.7327997821371393 and parameters: {'learning_rate': 0.01699667687871367, 'n_estimators': 3000}. Best is trial 28 with value: 0.7338267199650517.\n",
      "[I 2025-05-16 13:59:17,189] Trial 42 finished with value: 0.7339003280574057 and parameters: {'learning_rate': 0.03811999537709678, 'n_estimators': 3000}. Best is trial 42 with value: 0.7339003280574057.\n",
      "[I 2025-05-16 14:10:11,894] Trial 45 finished with value: 0.7338736235577977 and parameters: {'learning_rate': 0.03875926428005526, 'n_estimators': 3000}. Best is trial 42 with value: 0.7339003280574057.\n",
      "[I 2025-05-16 14:10:23,464] Trial 44 finished with value: 0.7331426404734656 and parameters: {'learning_rate': 0.018841053043303527, 'n_estimators': 3000}. Best is trial 42 with value: 0.7339003280574057.\n",
      "[I 2025-05-16 14:13:48,898] Trial 46 finished with value: 0.7338918300578927 and parameters: {'learning_rate': 0.038797245605525825, 'n_estimators': 3000}. Best is trial 42 with value: 0.7339003280574057.\n",
      "[I 2025-05-16 14:15:56,711] Trial 47 finished with value: 0.7332410613847932 and parameters: {'learning_rate': 0.019774819579172192, 'n_estimators': 3000}. Best is trial 42 with value: 0.7339003280574057.\n",
      "[I 2025-05-16 14:16:21,145] Trial 48 finished with value: 0.7338257376816102 and parameters: {'learning_rate': 0.039501034871276354, 'n_estimators': 3000}. Best is trial 42 with value: 0.7339003280574057.\n",
      "[I 2025-05-16 14:26:32,947] Trial 49 finished with value: 0.7331924325300729 and parameters: {'learning_rate': 0.019196530223745553, 'n_estimators': 3000}. Best is trial 42 with value: 0.7339003280574057.\n",
      "Setting n_estimators based on early stopping: 2765\n",
      "\n",
      "--- Final Tuning Results ---\n",
      "✅ Final best parameters after all stages:\n",
      "{'max_depth': 4, 'min_child_weight': 1.123473602060215, 'gamma': 1.3432017845835642, 'reg_alpha': 0.9892855839750394, 'reg_lambda': 0.2097958169371025, 'subsample': 0.8980386684264224, 'colsample_bytree': 0.9088349516071941, 'learning_rate': 0.03811999537709678, 'n_estimators': 2765}\n",
      "✅ Best AUC from Stage 4 (final): 0.7339\n",
      "\n",
      "To train the final model, use all parameters in `final_best_params`:\n",
      "Example: final_model = xgb.XGBClassifier(**final_best_params, random_state=RANDOM_SEED, n_jobs=XGB_N_JOBS)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Stage 4: Tuning learning_rate (and n_estimators via early stopping) ---\")\n",
    "\n",
    "def objective_stage4(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': 'gbtree',\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': XGB_N_JOBS,\n",
    "        # 'tree_method': 'hist',\n",
    "\n",
    "        # Parameters from previous stages (fixed)\n",
    "        'max_depth': current_best_params['max_depth'],\n",
    "        'min_child_weight': current_best_params['min_child_weight'],\n",
    "        'gamma': current_best_params['gamma'],\n",
    "        'reg_alpha': current_best_params['reg_alpha'],\n",
    "        'reg_lambda': current_best_params['reg_lambda'],\n",
    "        'subsample': current_best_params['subsample'],\n",
    "        'colsample_bytree': current_best_params['colsample_bytree'],\n",
    "\n",
    "        # Parameters to tune in Stage 4\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True), # Finer search for LR\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 3000, step=100), # Tune n_estimators directly or rely on early stopping with a large fixed value\n",
    "        'early_stopping_rounds': 100                                            # If tuning n_estimators directly, early_stopping_rounds should still be used.\n",
    "    }\n",
    "    \n",
    "    # It's often better to set a high n_estimators and let early stopping find the optimal number\n",
    "    # based on the learning rate. So, an alternative for this stage:\n",
    "    # params['n_estimators'] = 3000 # Fixed high value\n",
    "    # And you wouldn't suggest 'n_estimators' in the trial.\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=RANDOM_SEED)\n",
    "    aucs = []\n",
    "    actual_n_estimators_per_fold = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_os, y_os):\n",
    "        X_train_fold, X_valid_fold = X_os.iloc[train_idx], X_os.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y_os.iloc[train_idx], y_os.iloc[valid_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                  verbose=False)\n",
    "        \n",
    "        preds_proba = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_valid_fold, preds_proba)\n",
    "        aucs.append(auc)\n",
    "        actual_n_estimators_per_fold.append(model.best_iteration + 1 if model.best_iteration is not None else params['n_estimators']) # XGBoost best_iteration is 0-indexed\n",
    "\n",
    "    # Log the average actual n_estimators if using early stopping with a fixed large n_estimators\n",
    "    avg_actual_n_estimators = np.mean(actual_n_estimators_per_fold)\n",
    "    trial.set_user_attr(\"avg_actual_n_estimators\", avg_actual_n_estimators) # Store for later analysis\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "study_stage4 = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_stage4.optimize(objective_stage4, n_trials=N_TRIALS_PER_STAGE, n_jobs=OPTUNA_N_JOBS, show_progress_bar=True)\n",
    "\n",
    "best_params_stage4 = study_stage4.best_params\n",
    "final_best_params = current_best_params.copy()\n",
    "final_best_params.update(best_params_stage4)\n",
    "\n",
    "# If n_estimators was fixed high and determined by early stopping in Stage 4:\n",
    "# You might want to set n_estimators in final_best_params to the average actual n_estimators from the best trial.\n",
    "best_trial_stage4 = study_stage4.best_trial\n",
    "if 'avg_actual_n_estimators' in best_trial_stage4.user_attrs:\n",
    "    final_best_params['n_estimators'] = int(round(best_trial_stage4.user_attrs['avg_actual_n_estimators']))\n",
    "    print(f\"Setting n_estimators based on early stopping: {final_best_params['n_estimators']}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Tuning Results ---\")\n",
    "print(\"✅ Final best parameters after all stages:\")\n",
    "print(final_best_params)\n",
    "print(f\"✅ Best AUC from Stage 4 (final): {study_stage4.best_value:.4f}\")\n",
    "\n",
    "print(\"\\nTo train the final model, use all parameters in `final_best_params`:\")\n",
    "print(\"Example: final_model = xgb.XGBClassifier(**final_best_params, random_state=RANDOM_SEED, n_jobs=XGB_N_JOBS)\")\n",
    "# print(\"Then fit it on your full training data (X_os, y_os if it's the full training set, or X_train, y_train)\")\n",
    "# print(\"final_model.fit(X_os, y_os)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfaae37a-1c2a-4160-bdbb-98dd42bba05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(**final_best_params, random_state=RANDOM_SEED, n_jobs=XGB_N_JOBS, eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d200043a-4e96-4e5d-bab2-18a5b2371eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_os, y_os,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_os  # 保持类别分布一致\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "055065c9-5b6e-42c6-9c86-743dc376447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train the final XGBoost model...\n",
      "Final parameters for XGBoost model: {'max_depth': 4, 'min_child_weight': 1.123473602060215, 'gamma': 1.3432017845835642, 'reg_alpha': 0.9892855839750394, 'reg_lambda': 0.2097958169371025, 'subsample': 0.8980386684264224, 'colsample_bytree': 0.9088349516071941, 'learning_rate': 0.03811999537709678, 'n_estimators': 2765, 'early_stopping_rounds': 100, 'random_state': 42, 'n_jobs': 5, 'objective': 'binary:logistic', 'eval_metric': 'auc'}\n",
      "\n",
      "Training final model on X_os and y_os...\n",
      "\n",
      "Training final model...\n",
      "Using 574103 samples for training and 63790 for early stopping evaluation.\n",
      "[0]\tvalidation_0-auc:0.68966\n",
      "[200]\tvalidation_0-auc:0.72287\n",
      "[400]\tvalidation_0-auc:0.72691\n",
      "[600]\tvalidation_0-auc:0.72886\n",
      "[800]\tvalidation_0-auc:0.73000\n",
      "[1000]\tvalidation_0-auc:0.73088\n",
      "[1200]\tvalidation_0-auc:0.73149\n",
      "[1400]\tvalidation_0-auc:0.73208\n",
      "[1600]\tvalidation_0-auc:0.73242\n",
      "[1800]\tvalidation_0-auc:0.73274\n",
      "[2000]\tvalidation_0-auc:0.73292\n",
      "[2200]\tvalidation_0-auc:0.73308\n",
      "[2400]\tvalidation_0-auc:0.73319\n",
      "[2579]\tvalidation_0-auc:0.73318\n",
      "\n",
      "Final model training complete.\n",
      "Model was trained for 2765 rounds.\n",
      "Training stopped at 2765 rounds (n_estimators limit or other condition). Best iteration reported by XGBoost (0-indexed): 2479.\n",
      "\n",
      "Model saved successfully to final_xgboost_model.ubj (XGBoost native UBJ format)\n",
      "Model saved successfully to final_xgboost_model.joblib (joblib format)\n",
      "Model saved successfully to final_xgboost_model.pkl (pickle format)\n"
     ]
    }
   ],
   "source": [
    "import joblib # For saving/loading model (optional method)\n",
    "import pickle # For saving/loading model (optional method)\n",
    "\n",
    "print(\"Preparing to train the final XGBoost model...\")\n",
    "\n",
    "# 1. Determine the early_stopping_rounds value\n",
    "chosen_early_stopping_rounds = 100 # For example, set to 100 rounds. Adjust as needed.\n",
    "\n",
    "# 2. Prepare the final parameters dictionary\n",
    "params_for_final_model = final_best_params.copy() # Copy to avoid modifying the original dictionary (if needed)\n",
    "params_for_final_model['early_stopping_rounds'] = chosen_early_stopping_rounds\n",
    "params_for_final_model['random_state'] = RANDOM_SEED # Ensure other necessary fixed parameters are also present\n",
    "params_for_final_model['n_jobs'] = XGB_N_JOBS      # For XGBoost itself\n",
    "\n",
    "# Ensure 'objective' and 'eval_metric' are present for early stopping to work correctly\n",
    "if 'objective' not in params_for_final_model:\n",
    "    params_for_final_model['objective'] = 'binary:logistic' # Adjust according to your task\n",
    "if 'eval_metric' not in params_for_final_model:\n",
    "    # For binary classification AUC, 'auc' or 'logloss' are commonly used.\n",
    "    # 'auc' is more intuitive for early stopping. Ensure it's consistent with the metric used during optimization.\n",
    "    params_for_final_model['eval_metric'] = 'auc'\n",
    "\n",
    "print(f\"Final parameters for XGBoost model: {params_for_final_model}\")\n",
    "\n",
    "# 3. Initialize and train the final model\n",
    "final_model = xgb.XGBClassifier(**params_for_final_model)\n",
    "\n",
    "print(f\"\\nTraining final model on X_os and y_os...\")\n",
    "# When training on the full dataset with early stopping, and no explicit eval_set is provided,\n",
    "# XGBoost automatically splits a part of the training data as an internal validation set.\n",
    "# If you don't want this behavior, and n_estimators is already the optimal value\n",
    "# determined by cross-validation, you might consider not using early_stopping_rounds\n",
    "# for the final fit, or provide a specific eval_set (if applicable).\n",
    "# However, generally, using early stopping (with an internal validation set) during\n",
    "# the final fit is an additional safeguard against overfitting.\n",
    "\n",
    "# 4. Split X_os, y_os into a training part and an evaluation part for early stopping\n",
    "# You can choose the test_size (e.g., 0.1 for 10% validation, 0.2 for 20%)\n",
    "# Use stratify=y_os for classification tasks to maintain class proportions\n",
    "X_train_final_fit, X_eval_final_fit, y_train_final_fit, y_eval_final_fit = train_test_split(\n",
    "    X_os, y_os, test_size=0.1, random_state=RANDOM_SEED, stratify=y_os\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining final model...\")\n",
    "print(f\"Using {len(X_train_final_fit)} samples for training and {len(X_eval_final_fit)} for early stopping evaluation.\")\n",
    "\n",
    "# 5. Fit the model, providing the explicit eval_set\n",
    "final_model.fit(X_train_final_fit, y_train_final_fit,\n",
    "                eval_set=[(X_eval_final_fit, y_eval_final_fit)], # Pass the evaluation set\n",
    "                verbose=200) # Or verbose=100, or False\n",
    "\n",
    "print(\"\\nFinal model training complete.\")\n",
    "\n",
    "# 4. Check if early stopping was triggered and the best iteration\n",
    "# get_num_boosting_rounds() returns the actual number of trees built (equivalent to best_iteration + 1 if early stopping occurred)\n",
    "actual_boosting_rounds = final_model.get_num_boosting_rounds()\n",
    "print(f\"Model was trained for {actual_boosting_rounds} rounds.\")\n",
    "\n",
    "if hasattr(final_model, 'best_iteration') and final_model.best_iteration is not None:\n",
    "    # Note: best_iteration is 0-indexed, so the actual number of trees for the best model is best_iteration + 1\n",
    "    # This should match actual_boosting_rounds if use_best_model=True (which is implicit with early stopping)\n",
    "    if actual_boosting_rounds == final_model.best_iteration + 1:\n",
    "        print(f\"Early stopping was triggered. Best iteration was {final_model.best_iteration + 1}.\")\n",
    "    else:\n",
    "        # This case might occur if n_estimators was reached before early stopping criteria were met for 'early_stopping_rounds'\n",
    "        print(f\"Training stopped at {actual_boosting_rounds} rounds (n_estimators limit or other condition). Best iteration reported by XGBoost (0-indexed): {final_model.best_iteration}.\")\n",
    "else:\n",
    "    print(f\"Early stopping might not have been triggered, or n_estimators limit was reached.\")\n",
    "\n",
    "# --- Save the model ---\n",
    "model_filename_ubj = \"final_xgboost_model.ubj\"\n",
    "model_filename_joblib = \"final_xgboost_model.joblib\"\n",
    "model_filename_pkl = \"final_xgboost_model.pkl\"\n",
    "\n",
    "\n",
    "# Save using XGBoost native method\n",
    "try:\n",
    "    final_model.save_model(model_filename_ubj)\n",
    "    print(f\"\\nModel saved successfully to {model_filename_ubj} (XGBoost native UBJ format)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving model with XGBoost native save_model: {e}\")\n",
    "\n",
    "# Save using joblib\n",
    "try:\n",
    "    joblib.dump(final_model, model_filename_joblib)\n",
    "    print(f\"Model saved successfully to {model_filename_joblib} (joblib format)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving model with joblib: {e}\")\n",
    "\n",
    "# Save using pickle\n",
    "try:\n",
    "    with open(model_filename_pkl, \"wb\") as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    print(f\"Model saved successfully to {model_filename_pkl} (pickle format)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving model with pickle: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835ee1f-dc94-4ed3-b9df-2c6573291f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
